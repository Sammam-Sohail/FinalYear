{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from binance import Client\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import seaborn as sns\n",
    "from deep_translator import GoogleTranslator\n",
    "import datetime as dt\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "sns.set() # use seaborn plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLIC_KEY = 'WhihdXQmvY9QAObIELyVGAI3h1nvJZDYg1B68H7n0MHn0bISKXMwHozMcxMbaJ0Z'\n",
    "PRIVATE_KEY = '2auNidRhbGrAG06UWK0MPcPYJtBj9fMFKpAWJsyjTACJLHHIdOHVbsLj8MZopJJ4'\n",
    "client = Client(PUBLIC_KEY, PRIVATE_KEY)\n",
    "tickers = client.get_all_tickers()\n",
    "df = pd.DataFrame()\n",
    "for coins in client.get_all_tickers():\n",
    "    if((coins[\"symbol\"][-4:]==\"BUSD\")):\n",
    "        historical_data = client.get_historical_klines(\n",
    "            coins[\"symbol\"], Client.KLINE_INTERVAL_1DAY, '19 July 2022')\n",
    "        coin_df = pd.DataFrame(historical_data,columns = ['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time',\n",
    "                'Quote Asset Volume', 'Number of Trades', 'TB Base Volume', 'TB Quote Volume', 'Ignore'])\n",
    "        coin_df[\"COIN\"]=coins[\"symbol\"][:-4]\n",
    "        df = pd.concat([df,coin_df],axis=0)\n",
    "df['Open Time'] = pd.to_datetime(df['Open Time']/1000, unit='s')\n",
    "df['Close Time'] = pd.to_datetime(df['Close Time']/1000, unit='s')\n",
    "numeric_columns = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                   'Quote Asset Volume', 'TB Base Volume', 'TB Quote Volume']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, axis=1)\n",
    "df[\"Change\"] = ((df[\"High\"]-df[\"Low\"])/df[\"Low\"]).apply(lambda x: x*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=\"Change\",ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list1 = []\n",
    "username_not_needed = ['dashfit_signal','moon_or_earth', 'Crypto3OT', 'abnormal_crypto']\n",
    "begin_date = dt.date(2022, 6, 18)\n",
    "end_date = dt.date(2022, 7, 18)\n",
    "# >30 is not good do something else\n",
    "for coin in df.loc[df[\"Change\"] > 30, \"COIN\"]:\n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper('#'+coin, begin_date, end_date).get_items()):\n",
    "        if ((i < 1000)):\n",
    "            if((tweet.user.username not in username_not_needed)):\n",
    "                (tweets_list1.append([tweet.date, tweet.user.username,coin,GoogleTranslator('auto', 'en').translate(tweet.content)]))\n",
    "        else:\n",
    "            break\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Username', 'Coin', 'Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "# current_date =date.today()\n",
    "def text_cleaning(text):\n",
    "    stop_free = ' '.join([word for word in text.lower().split() if word not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = ' '.join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "    return normalized.split()\n",
    "\n",
    "tweets_df1[\"Cleaned Text\"]= tweets_df1[\"Text\"].apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_df1 = pd.read_csv(\"19.csv\")\n",
    "# tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Untitled spreadsheet - 11.csv\")\n",
    "df1 = df.drop(columns=[\"Unnamed: 0\",\"Username\",\"Coin\",\"Text\",\"Datetime\"])\n",
    "X = df1.iloc[:,1]\n",
    "Y = df1.iloc[:,0]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=.2)\n",
    "model = make_pipeline(TfidfVectorizer(lowercase=False), MultinomialNB())\n",
    "model.fit(X_train, Y_train)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predictions(my_sentence, model):\n",
    "    prediction = model.predict([my_sentence])\n",
    "    return prediction\n",
    "result=list()\n",
    "# tweets_df1[\"Cleaned Text\"].apply(lambda x: x.lower())\n",
    "for i in tweets_df1[\"Cleaned Text\"]:\n",
    "    result.append(my_predictions(str(i),model))\n",
    "tweets_df1[\"Result\"]=result\n",
    "tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df1.to_csv(\"19 test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (70, 20)})\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"Username\", data=tweets_df1)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df1[\"Username\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df1[\"Cleaned Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clean the data\n",
    "# stop = set(stopwords.words('english'))\n",
    "# exclude = set(string.punctuation)\n",
    "# lemma = WordNetLemmatizer()\n",
    "\n",
    "# def clean(text):\n",
    "#     stop_free = ' '.join([word for word in text.lower().split() if word not in stop])\n",
    "#     punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "#     normalized = ' '.join([lemma.lemmatize(word) for word in punc_free.split()])\n",
    "#     return normalized.split()\n",
    "\n",
    "# df['text_clean']=df['text'].apply(clean)\n",
    "#create dictionary\n",
    "# dictionary = corpora.Dictionary(tweets_df1['Cleaned Text'])\n",
    "#Total number of non-zeroes in the BOW matrix (sum of the number of unique words per document over the entire corpus).\n",
    "# print(dictionary.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=df[\"Date\"])\n",
    "df = df[df[\"Change\"]>30]\n",
    "for i in list(set(df[\"COIN\"])):\n",
    "    print(i)\n",
    "    # dict(zip(i, values))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
